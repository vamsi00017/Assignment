{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Simple Linear Regression**\n",
        "\n",
        "\n",
        " Simple Linear Regression is a statistical method that models the relationship between a dependent variable (Y) and an independent variable (X) using the equation Y = mX + c, where 'm' is the slope and 'c' is the intercept."
      ],
      "metadata": {
        "id": "oog4Tjav6yuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "* Linearity: The relationship between X and Y is linear.\n",
        "* Independence: Observations are independent.\n",
        "* Homoscedasticity: Constant variance of residuals.\n",
        "* Normality: Residuals follow a normal distribution.\n",
        "* No multicollinearity: Only one independent variable.\n"
      ],
      "metadata": {
        "id": "bXxETi8y8tYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What does the coefficient m represent in the equation Y=mX+c?**\n",
        "\n",
        "The coefficient 'm' represents the slope of the regression line, indicating the rate of change in Y for a one-unit increase in X."
      ],
      "metadata": {
        "id": "5KEGnFvH8tWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What does the intercept c represent in the equation Y=mX+c?**\n",
        "\n",
        "The intercept 'c' represents the value of Y when X is zero. It is the point where the regression line crosses the Y-axis.\n"
      ],
      "metadata": {
        "id": "CyEmUmik8tS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "m = (Σ(X - mean(X)) * (Y - mean(Y))) / Σ(X - mean(X))²"
      ],
      "metadata": {
        "id": "vhRnGv6_8tQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "The least squares method minimizes the sum of squared differences between observed and predicted Y values to find the best-fitting regression line."
      ],
      "metadata": {
        "id": "mLeZM_EZ8tNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "R² measures the proportion of variance in Y explained by X. A value close to 1 indicates a strong relationship, while a value near 0 suggests a weak relationship."
      ],
      "metadata": {
        "id": "XEnFZjU48tKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Multiple Linear Regression?**\n",
        "\n",
        "Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables using Y = b0 + b1X1 + b2X2 + ... + bnXn."
      ],
      "metadata": {
        "id": "ULnZOpcx8tHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "Simple Linear Regression has one independent variable, while Multiple Linear Regression has two or more independent variables."
      ],
      "metadata": {
        "id": "ZdsnNpgl8tEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "* Linearity\n",
        "* Independence\n",
        "* Homoscedasticity\n",
        "* Normality\n",
        "* No multicollinearity (independent variables should not be highly correlated)."
      ],
      "metadata": {
        "id": "sGeFqskD8tBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "Heteroscedasticity occurs when residual variance is not constant, leading to unreliable standard errors and affecting hypothesis tests."
      ],
      "metadata": {
        "id": "rh5W1__i8s-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "* Remove highly correlated variables\n",
        "* Use Principal Component Analysis (PCA)\n",
        "* Apply Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "vNu1RMDq8s7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "* One-Hot Encoding\n",
        "* Label Encoding\n",
        "* Target Encoding"
      ],
      "metadata": {
        "id": "wssWrU9P8s4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "Interaction terms capture combined effects of independent variables that impact Y beyond their individual contributions."
      ],
      "metadata": {
        "id": "LThOMIyA8s1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "In Simple Linear Regression, the intercept is the Y value when X is zero. In Multiple Linear Regression, it represents Y when all Xs are zero, which may not always be meaningful."
      ],
      "metadata": {
        "id": "VGgbgBcc8sy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "The slope represents the rate of change in Y per unit increase in X. It determines the direction and strength of the relationship."
      ],
      "metadata": {
        "id": "KPN4yd9y8svx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "The intercept helps establish a reference point for Y when all predictors are at zero, providing baseline insights."
      ],
      "metadata": {
        "id": "xgf0oMQA8ss4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are the limitations of using R² as a sole measure of model performance?**\n",
        "R² does not indicate causation, does not work well with nonlinear relationships, and can be misleading in complex models."
      ],
      "metadata": {
        "id": "DdKsOOAg8spz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "A large standard error suggests high variability in coefficient estimation, indicating low reliability."
      ],
      "metadata": {
        "id": "RjENoYWe8snJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "Heteroscedasticity appears as a fan-shaped pattern in residual plots. It must be addressed for reliable coefficient estimates."
      ],
      "metadata": {
        "id": "KGiAPdey8skB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "It indicates that adding independent variables may not be contributing meaningful explanatory power, causing overfitting."
      ],
      "metadata": {
        "id": "gDLFfjNW8shN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "Scaling ensures variables contribute equally, preventing dominance by large-scale features."
      ],
      "metadata": {
        "id": "yjQsPApe8seT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is polynomial regression?**\n",
        "\n",
        "Polynomial Regression is an extension of linear regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "u4N6RWRV8sbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How does polynomial regression differ from linear regression?**\n",
        "\n",
        "Polynomial Regression captures nonlinear relationships by including higher-degree terms of the independent variable."
      ],
      "metadata": {
        "id": "3k0CtVO_8sYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When is polynomial regression used?**\n",
        "\n",
        "It is used when data shows a curvilinear trend that a linear model cannot capture."
      ],
      "metadata": {
        "id": "lcIrsI1i8sVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is the general equation for polynomial regression?**\n",
        "\n",
        "Y = b0 + b1X + b2X² + ... + bnXⁿ"
      ],
      "metadata": {
        "id": "-reLuOQ08sSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "Yes, it can be extended to multiple independent variables, leading to Polynomial Multiple Regression."
      ],
      "metadata": {
        "id": "VRBED1XQ8sQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are the limitations of polynomial regression?**\n",
        "\n",
        "* Overfitting with high-degree polynomials\n",
        "* Computationally expensive\n",
        "* Sensitive to outliers."
      ],
      "metadata": {
        "id": "qk6zZHw58sPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "* R² Score\n",
        "* Cross-validation\n",
        "* Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "SV3lmIBW8r_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why is visualization important in polynomial regression?**\n",
        "\n",
        "Visualization helps identify the polynomial degree required to capture patterns effectively."
      ],
      "metadata": {
        "id": "tHYDcjnn8r8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How is polynomial regression implemented in Python?**"
      ],
      "metadata": {
        "id": "tY3T8XzII6kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "y = np.array([2.1, 2.9, 3.8, 5.2, 6.8, 8.7, 11.1, 13.9, 17.5, 21.2])\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label=\"Data\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial Regression\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"X Axis\")\n",
        "plt.ylabel(\"Y Aix\")\n",
        "plt.title(\"Polynomial Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O0xdePFuI_Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBj3m4WxJPF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}